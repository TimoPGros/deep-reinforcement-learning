{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-Structure\n",
    "The Project-Structure is manly oriented to the LunarLander-project. There are 3 different classes:\n",
    "- Network\n",
    "- Agent\n",
    "- ReplayBuffer\n",
    "\n",
    "### Network\n",
    "This class represents the neural network used as a policy. It consists of two hidden layers, each having 64 nodes, and an output layer with 4 nodes. The input layer has 37 nodes. \n",
    "\n",
    "### ReplayBuffer\n",
    "Similar to the LunarLander-project, the Replay-Buffer consists of a memory with fixed maximum size and gives the opportunity to add entries and to sample from the saved entries. \n",
    "\n",
    "### Agent\n",
    "This agents is an implementation of the DQN algorithm, includind Experience Replay and Fixed Targets. \n",
    "\n",
    "### Hyperparameter\n",
    "There are various different Hyperparameters. For the provided result, the parameters \n",
    "- BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "- BATCH_SIZE = 64         # minibatch size\n",
    "- GAMMA = 0.99          # discount factor\n",
    "- TAU = 0.001#1e-3              # for soft update of target parameters\n",
    "- LR = 5e-4               # learning rate \n",
    "- UPDATE_EVERY = 4        # how often to update the network\n",
    "- eps_start = 1.0\n",
    "- eps_end = 0.0001\n",
    "- eps_decay = 0.995\n",
    "- n_episode = 2000\n",
    "- max_t = 2500\n",
    "\n",
    "where chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The implemented algorithm can be found unter dqn and samples n_episodes many episodes with length t_max from the environment. It calls the agents step method to save the samples and to train the agent. Furthermore, the return of the last 100 episodes is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "One possibility to further improve the algorithm would be to implement Prioritized Experience Replay. I started a short extension to Prioritized Experience Replay, having a really bad runtime. When implementing just that, one has to take great care on the way to save the priorities. \n",
    "\n",
    "Another option for improvement would be another neural network. \n",
    "\n",
    "On top of that, a grid search could find the optimal Hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
